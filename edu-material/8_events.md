# The Neuroscience of Sensors
LLMs and modern deep learning is pretty damn expensive,
The brain, on the other hand, is optimized to process data far more cheaply. 
Before data reaches the brain, we have a bunch of sensors that convert the world around us into a cheaper form of data.
Our eyes, nose, ears, fingers, various other body parts, are for sensing. How do our sensors interface with our brain to sense the world around us? How can we make use of it in our software and hardware?

The next couple of weeks tap into more exploratory technologies - breaking away from the commonly accepted techniques in deep learning, and into the realm of neuromorphic computing.

## Background material
* [Event-based vision: A Survey](https://arxiv.org/abs/1904.08405)
* [Training Spiking Neural Networks uUsing Lessons from Deep Learning: Sections 1 & 2](https://arxiv.org/abs/2109.12894) - guess who wrote this paper hehe~

## Coding Exercises
* [Spike Encoding with snnTorch](https://snntorch.readthedocs.io/en/latest/tutorials/tutorial_1.html)
* [Event-based Sensors Tutorial]( https://github.com/gcohen/AMOS-Short-Course)

This last tutorial is a very hands-on, applications driven view to working with event-cameras.