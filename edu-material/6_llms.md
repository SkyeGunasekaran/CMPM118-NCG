# Large Language Models

Goddamn, there is so much that has been developed in LLMs in such a short amount of time. 

## Transformer Basics
* [Andrej Karpathy: A generalist intro to LLMs](https://youtu.be/zjkBMFhNj_g?si=yOw8U-5hXhHBNPHK) - 
* [3Blue1Brown Videos: Lecture 5-8](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
* [My own tutorial on LLMs: Part 1](https://youtu.be/acSIdUxku4w?si=84qjmnw4u3lidQMB) - I tap into a few more advanced principles here. 
* [My own tutorial on LLMs: Part 2](https://youtu.be/C0R-V11elkY?si=ZdryEtF377rAvxIs) - More research-oriented.
* [Build a Transformer from Scratch](https://youtu.be/kCc8FmEb1nY?si=-2GL87MZp-gkZ3V3)

## More Advanced Concepts in Language Modeling
* [Flash Attention from Scratch](https://www.youtube.com/watch?v=zy8ChVd_oTM&t=6304s&ab_channel=UmarJamil)
* [DeepSeek](https://www.youtube.com/watch?v=0VLAoVGf_74&ab_channel=WelchLabs)
* [Low Precision LLMs](https://www.youtube.com/playlist?list=PL4bm2lr9UVG0HvePBXvsceO4yuLC8HhUh)
* [Reinforcement Learning in LLMs](https://www.youtube.com/playlist?list=PL4bm2lr9UVG2EFRlZ7_kzp6yiJdsOikfe) - training an LLM is a multi-stage process. After doing next-token prediction, they are then further optimized via another training process based on rewards for correct answers to well-defined questions.
* [Reasoning Models](https://arxiv.org/abs/2507.06203) - a paper led by my graduate student